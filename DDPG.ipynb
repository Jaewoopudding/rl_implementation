{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtqdm\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m#from ddpg import DDPGAgent\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#from ddpg import DDPGAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "LR = 0.001\n",
    "TAU = 0.001\n",
    "GAMMA = 0.99\n",
    "EPOCHS = 100\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "env = gym.make('Pendulum-v1', g=9.81)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGCritic(nn.Module):\n",
    "    def __init__(self, num_state, num_action, n_hidden_action=16, n_hidden_state=32, n_hidden=256):\n",
    "        super().__init__()\n",
    "        self.state_net  = nn.Linear(in_features=num_state, out_features=n_hidden_state)\n",
    "        self.action_net = nn.Linear(in_features=num_action, out_features=n_hidden_action)\n",
    "        self.linear1 = nn.Linear(in_features=(n_hidden_state + n_hidden_action), out_features=n_hidden)\n",
    "        self.linear2 = nn.Linear(in_features=n_hidden, out_features=n_hidden)\n",
    "        self.linear3 = nn.Linear(in_features=n_hidden, out_features=1)\n",
    "        self.relu    = nn.ReLU()\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        s_embedding = self.relu(self.state_net(state))\n",
    "        a_embedding = self.relu(self.action_net(action))\n",
    "        embedding = torch.cat([s_embedding, a_embedding], dim=-1)\n",
    "        out = self.relu(self.linear1(embedding))\n",
    "        out = self.relu(self.linear2(out))\n",
    "        out = self.linear3(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class DDPGActor(nn.Module):\n",
    "    def __init__(self, num_state, n_hidden=256, bound:tuple = None):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(in_features=num_state, out_features=n_hidden)\n",
    "        self.linear2 = nn.Linear(in_features=n_hidden, out_features=n_hidden)\n",
    "        self.linear3 = nn.Linear(in_features=n_hidden, out_features=1)\n",
    "        self.relu    = nn.ReLU()\n",
    "        self.tanh    = nn.Tanh()\n",
    "        self.bound   = bound\n",
    "        \n",
    "    def forward(self, state):\n",
    "        out = self.relu(self.linear1(state))\n",
    "        out = self.relu(self.linear2(out))\n",
    "        out = self.tanh(self.linear3(out))\n",
    "        if self.bound:\n",
    "            out = torch.clip(out, min=self.bound[0], max=self.bound[1])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import ReplayMemory, Transition\n",
    "import random, math\n",
    "from itertools import count\n",
    "import tqdm\n",
    "\n",
    "class DDPGAgent(torch.nn.Module):\n",
    "    def __init__(self, env, batch_size, tau, gamma, device, critic_lr=1e-4, actor_lr=1e-3, memory_size=10e6):\n",
    "        super().__init__()\n",
    "        self.env = env\n",
    "        self.batch_size = batch_size\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "        \n",
    "        self.num_state = env.observation_space.shape[0]\n",
    "        self.num_action= env.action_space.shape[0]\n",
    "        self.lower_bound, self.upper_bound = env.action_space.low[0], env.action_space.high[0]\n",
    "        \n",
    "        self.critic        = DDPGCritic(self.num_state, self.num_action)\n",
    "        self.target_critic = DDPGCritic(self.num_state, self.num_action)\n",
    "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
    "        \n",
    "        self.actor        = DDPGActor(self.num_state, bound=(self.lower_bound, self.upper_bound))\n",
    "        self.target_actor = DDPGActor(self.num_state, bound=(self.lower_bound, self.upper_bound))\n",
    "        self.target_actor.load_state_dict(self.actor.state_dict())\n",
    "        \n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "        self.actor_optimizer  = torch.optim.Adam(self.actor.parameters(),  lr=actor_lr)\n",
    "        \n",
    "        self.memory = ReplayMemory(memory_size)\n",
    "        self.steps_done = 0\n",
    "        self.critic_loss_history = []\n",
    "        self.actor_loss_history = []\n",
    "        self.critic_criterion = nn.MSELoss()\n",
    "            \n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        batch=Transition(*zip(*transitions))\n",
    "\n",
    "        next_state, state, action, reward = list(map(torch.cat, [batch.next_state, batch.state, batch.action, batch.reward]))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            target_q = (self.target_critic(next_state, self.target_actor(next_state)) * self.gamma) + reward\n",
    "\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss = self.critic_criterion(self.critic(state, action), target_q)\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        self.critic_loss_history.append(critic_loss)\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss = -self.critic(state, self.actor(state)).mean()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        print(actor_loss)\n",
    "        self.actor_loss_history.append(actor_loss)\n",
    "        \n",
    "        \n",
    "    def train(self, episodes):\n",
    "        try: \n",
    "            assert torch.cuda.is_available()\n",
    "            num_episodes = episodes\n",
    "        except:\n",
    "            print(\"CUDA Unavailable\")\n",
    "            num_episodes = 50\n",
    "            \n",
    "        for _ in tqdm.tqdm(range(num_episodes), ncols=100):\n",
    "            state, _ = self.env.reset()\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "            for _ in count():\n",
    "                action = self.actor(state)\n",
    "                observation, reward, terminated, truncated, _ = self.env.step([action.item()])\n",
    "                done = terminated or truncated\n",
    "                if terminated:\n",
    "                    next_state = None\n",
    "                else:\n",
    "                    next_state = torch.tensor(observation, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "                self.memory.push(state, action, next_state, torch.tensor(reward, dtype=torch.float32).unsqueeze(0).unsqueeze(0))\n",
    "                state = next_state\n",
    "                \n",
    "                self.optimize_model()\n",
    "                \n",
    "                self.target_actor.load_state_dict(self.soft_update(self.actor, self.target_actor))\n",
    "                self.target_critic.load_state_dict(self.soft_update(self.critic, self.target_critic))\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "    def soft_update(self, network, target_network):\n",
    "        network_dict = network.state_dict()\n",
    "        target_dict = target_network.state_dict()\n",
    "        for key in network_dict:\n",
    "            target_dict[key] = (1-self.tau) * target_dict[key] + self.tau * network_dict[key]\n",
    "        return target_dict\n",
    "                \n",
    "    def savefig(self, root=None):\n",
    "        durations_t = torch.tensor(self.episode_durations, dtype=torch.float)\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "        plt.title('Result')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Reward')\n",
    "        plt.savefig(root if root is None else 'ddpg.png')\n",
    "        plt.show()\n",
    "        \n",
    "            \n",
    "    def plot_duration(self, show_result=False):\n",
    "        plt.figure(1)\n",
    "        durations_t = torch.tensor(self.episode_durations, dtype=torch.float)\n",
    "        if show_result:\n",
    "            plt.title(\"Result\")\n",
    "        else:\n",
    "            plt.clf()\n",
    "            plt.title(\"Training\")\n",
    "            plt.xlabel('Episode')\n",
    "        plt.ylabel('Duration')\n",
    "        plt.plot(durations_t.numpy())\n",
    "        if len(durations_t) >= 100:\n",
    "            means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "            means = torch.cat((torch.zeros(99), means))\n",
    "            plt.plot(means.numpy())\n",
    "        plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Unavailable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▎                                                              | 1/50 [00:00<00:37,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1548, grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m agent \u001b[39m=\u001b[39m DDPGAgent(env\u001b[39m=\u001b[39menv, batch_size\u001b[39m=\u001b[39mBATCH_SIZE, gamma\u001b[39m=\u001b[39mGAMMA, tau\u001b[39m=\u001b[39mTAU, device\u001b[39m=\u001b[39mdevice, memory_size\u001b[39m=\u001b[39m\u001b[39m10000\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m agent\u001b[39m.\u001b[39;49mtrain(EPOCHS)\n",
      "Cell \u001b[0;32mIn[50], line 83\u001b[0m, in \u001b[0;36mDDPGAgent.train\u001b[0;34m(self, episodes)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39mpush(state, action, next_state, torch\u001b[39m.\u001b[39mtensor(reward, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m))\n\u001b[1;32m     81\u001b[0m state \u001b[39m=\u001b[39m next_state\n\u001b[0;32m---> 83\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimize_model()\n\u001b[1;32m     85\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_actor\u001b[39m.\u001b[39mload_state_dict(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoft_update(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactor, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_actor))\n\u001b[1;32m     86\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_critic\u001b[39m.\u001b[39mload_state_dict(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoft_update(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritic, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_critic))\n",
      "Cell \u001b[0;32mIn[50], line 49\u001b[0m, in \u001b[0;36mDDPGAgent.optimize_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritic_optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     48\u001b[0m critic_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritic_criterion(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritic(state, action), target_q)\n\u001b[0;32m---> 49\u001b[0m critic_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     50\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritic_optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     51\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritic_loss_history\u001b[39m.\u001b[39mappend(critic_loss)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/RL101/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/RL101/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "agent = DDPGAgent(env=env, batch_size=BATCH_SIZE, gamma=GAMMA, tau=TAU, device=device, memory_size=10000)\n",
    "agent.train(EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.clip([0.2], 0, 3)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.critic_loss_history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "introtorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
